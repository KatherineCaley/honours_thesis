\chapter{Discussion}

The results are contingent on the correct annotation of orthology. The human data set is the most curated DNA sequence data we have. choosing to use humans was to give the best possible chance.

\section{Nabla}
Such a substantial difference is a compelling result. Where there is strong evidence of the recent evolution of mutation, \textit{D. melanogaster}, the vast majority of genes are in mutation disequilibrium. In a closely related species where the state of mutation has not so drastically changed, \textit{D. simulans}, only half of the genes are in mutation disequilibrium. 

\section{RIP T-fiddy}

There was an implicit logic in the construction of T50 that the relationship between chronological time and substitution events would be the same. Now, there is a strong suspicion that this is not the case. 

This representation was not found to work for this problem because it violates the notion of a relationship between an event and chronological time being constant. 

T50 was motivated by what they do in chemistry with radioactive elements, the reason that this works for chemistry is that the rate of decay is constant through chronological time. This was the implicit assumption brought into the definition of $T_{50}$, which has turned out to be its failings. As a consequence the properties that it exhibited were paradoxical and it was not included in any empirical analyses. Further explanation is provided in the discussion. 


You are further away from equilibrium the further you go back in time. So you may expect that the further you are from equilibrium the further away you are from half-way. But we know that at the start of the process, more things change more quickly than at the end, in other words, the decay is exponential. So maybe there is further to go to the half way distribution (in terms of JSM), but you get there quicker (in terms of ENS) because down in the asymptotic tail, even if you are really close, because its an asymptote, it takes you forever to get there. And perhaps a change is just as likely to take you away from equilibrium that it is to take to towards equilibrium. 

The further you are from equilibrium the more of your steps will take you towards equilibrium.  As you get close to it, more of your steps will be noise that takes you away. That's the paradox. 

Experiment was to make up a random Q and pi and run it, and calculate T50 along each point along the curve. It shows that even though you are getting closer to equilibrium your t50 can be increasing. 

It demonstrates that its not obvious how to choose these metrics and your intuition is not always correct. 


\subsection*{Naturally occurring cases of the alternate hypothesis support the efficiency of the methods}


Although the expectation of the mutational process being radically different was not reflected by the temporal EOP test between the species, the results overwhelmingly support predictions made solely on knowledge of the biology. When DNA methylation, a well-established mutagenic effect is removed, there is a genome-wide increase in the existence and magnitude of mutation disequilibrium. 

This result validated the need to develop methods of quantifying mutation disequilibrium, for with this result alone, the X-specific and PAR-located regions are indistinguishable.

Thus, the current understanding of how mutagenesis affects the divergence process is reflected when perturbations happen and there is disequilibrium. Such a positive empirical control adds strength to the confidence in the developed measures and their effective detection and measurements of mutation disequilibrium. 

\section{Is the Human Genome at Equilibrium?}
'

Although, it may also be due to the intrinsic differences in the rate of evolution across the genome.

The question of whether the human genome is at mutation equilibrium is of interest to most humans. Unlike the previous empirical applications, there is no obvious mechanism providing an expectation of the level of mutation disequilibrium. But it is because of the 


The Quantile-Quantile plots suggest that there is considerable evidence for non-equilibriumness. What these results do not guarantee is that that non-equilibriumness is a consequence entirely of changes to mutagenesis. That is a presumption. There is confidence that there is substantial non-equlibriumness on the basis that the plots. acknowledging that we can never be certain because of Simpson's paradox, because what we are analysing is just a single dimension-ed thing. However, what I cannot say with certainty, is the origins. That the reason that there is departure is entirely due to changes in mutagenesis. I will have to make logical statements. 

logical statement: 

Demonstrated abundance of functional elements in the genome outside of protein coding sequence ->

Evidence of rate of evolution in intronic sequences is greater than the rate of evolution in exonic sequences from the primate analysis based on the length distribution (there's overlap but its mostly to the right of zero -> as you go out to more divergent primate the introns become less recognisably related to each other, indicating that the rate of evolution of introns is very strong -> so if we use conservation, in terms of preserving the permutation of nucleotides because it encodes function as an indicator of the operation of natural selection -> then it is greatly diminished within a relatively short time period (20 - million years) -> these are not definitive, but the argue against the abundance of natural selection being the force behind these transitions.

Impact of Natural Selection \cite{Graur2013OnENCODE}. Essentially, the questions remains open. 
low influence of natural selection in introns. 
The determination that selection is operating is often based on variability and rates of evolution. The underlying premise of that work, is often, that the neutral rate is uniform. The challenge to that is the strong evidence that mutagenesis is not uniform. As a consequence, those conclusions are in doubt, until we have a clearer understanding of what the true baseline of neutral processes look like. What this means is not that everything is neutral, just that until demonstrated otherwise, we cannot establish whether it is neutral or adaptive. 

There is a lack of evidence of selection operating outside of protein coding sequences. 
Introns are our neutral benchmark. 

Relying on correlations and inferences about historic processes is fraught, because we cant go back. You need evidence for functional elements, and the best evidence for that is direct genetic manipulation. This is the most compelling evidence.

The level of certainty in a statement I can make is:
It seems more likely that most genomic sequence, even within introns and/or between genes, is not functional. The size of motifs that proteins is quite modest (8-9 nts). 

My experimental design only considers mutagenic processes. Fxy and Drosophila are only about mutation. What we don't have is an explicit consideration of the influence of selection. But I have it indirectly in the form of the third codon position. I could look at 1st vs 3rd codon position to indirectly get at the selective influence. 

Can I rule out changes in selection as a factor in non-equilibriumness. We dont know how natural selection affects the magnitude of non-stationarity. There is almost certainly an inpact of selection, certainly in the protein coding sequence. But, considering the scale of functional elements relative to the length of the data, could pretty confidently rule it out as a major factor in introns. 


There is a proportion of the genome which is at equilibrium, according to the intronic results. There is a different proportion according to the exonic data. This tells you that the genome is heterogeneous in the distribution of non-equilibriumness. This raises a question, is this because of simply different rates of evolution towards the equilibrium across the entire genome, so this what ever has generated this disequilibrium? Or are these localised events? What are we seeing the results of. But I have some methods that can help to start tackle this. 


The assumption that the null hypothesis is uniformly distributed is a very important for the methods of estimating the amount of data that is consistent with the null to be appropriate. In the exonic data, that the null data does not fall on the diagonal line illustrates that there is a striking impact of the data not coming from the same distribution. The estimation procedures have that flax (require uniform distribution). The properties of the estimate are not entirely understood as a consequence of that, and how accurate that is going to be. The departure from theory is small enough that there is still something to be gained from this data, but it means that the estimate is imprecise (we do not know the impact on the procedure for estimating the proportion). 



What you could do, is randomly draw from the true negative and true positives in different proportions and ask how well that fits with your observed data, and try to solve for the fraction. 



\section{Drosophila}

 DNA methylation is a genome-wide phenomenon and therefore, the lack thereof is expected to have an effect on such a genome-wide scale.  
 
 
 Is the difference that we are seeing not actually due to the lack of methylation enzyme. well a way to support lack of methylation as a driving factor of the disequilibrium is to look at the compositions of the equilibrium distributions between Dmel and Dsim. because the mechanism actually has other predictions that we can verify. 

\section{Lynch}

If the lower bound on the mutation rate is effective population size, then I would expect to see lower levels of disequilibrium in population with low (or high can't remember) Ne. 
Theory: Natural selection operates on a mutator allele by the changes it makes elsewhere in the genome. 
The ability for natural selection to operate on the mutator alleles, and to select against them, depends on the changes made by the allele. 

The ability to act on the changes depends on the effective population size. 

Do we see a relationship between the magnitude of disequilibrium as measures by $\nabla$ and effective population if these methods were applied to a wider set of genomes. 

Bearing in mind that there are possible confounding factors - i.e., Dmel and Dsim have pretty similar effective population sizes, but radical change in mutagenesis in Dsim. 



\section{Future Directions}


The development of a method of visualising the direction of compositional change would provide a unique understanding of the processes acting upon the genome. As mentioned previously, there is an expectation of an increase in GC content associated with the loss of $^5$mC. $\delta_\nabla$ is the magnitude of the vector that represents the current rate of change of nucleotide composition. A vector also has an associated direction. Mapping the direction and magnitude of compositional change across the genome. The application of vector fields to areas such as developmental biology \citep{Steiner2009VectorEmbryogeny} and transcriptomics \citep{Qiu2021MappingCells}, represent a new and exciting context to understanding biological phenomena. 


\section{Conclusion}
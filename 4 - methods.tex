\chapter{Methods}

\section{Data Sets}

\subsubsection{Microbial}

To establish the properties of my methods on alignments of taxa that range in their evolutionary divergence, I chose a widely used molecular marker of genetic diversity. Herein the Microbial data, this is a subset of GreneGenes, a database of consistent alignments of the 16S rRNA gene in microbes \citep{McDonald2012AnArchaeab}. The sequences are aligned using a customised alignment algorithm for ribosomal RNA that uses knowledge of secondary structure and conserved residues \citep{McDonald2012AnArchaea}. The Microbial data consists of 9702 alignments of species triples that were sampled from GreneGenes to get uniform representation of maximum Jensen-Shannon Divergence (JSD), details of the sampling process are described in \citep{Kaehler2015}. An important side effect of the sampling process is that a species can appear in more than one triple, so not each alignment is independent. The Microbial data was downloaded from Dryad data (\href{https://doi.org/10.5061/dryad.g7g0n}{https://doi.org/10.5061/dryad.g7g0n}).  

\subsubsection{Drosophila}

In order to test for elevated mutation disequilibrium in \textit{D. melanogaster}, I sampled alignments of \textit{D. melanogaster} genes with orthologs from closely related taxa: \textit{D. simulans} and \textit{D. yakubra}. Herein the Drosophila data, this contains 9237 alignments of one-to-one orthologs of protein coding sequence (CDS) obtained from \textit{flyDIVaS}, a database of curated \textit{D. melanogaster}-centric orthologous gene sets \citep{Stanley2016FlyDIVaS:Selection, Clark2007EvolutionPhylogeny}. The \textit{flyDIVaS} pipeline extracts protein-coding genes from the latest FlyBase release, identifying one-to-one orthologs using OrthoDB \citep{Zdobnov2021OrthoDBOrthologs}. Alignments of proteins are then performed using MUSCLE \citep{Edgar2004MUSCLE:Complexity}, which are backtranslated to CDS, filtered and masked. Further details of the sampling process are described in \citep{Stanley2016FlyDIVaS:Selection}. 

\subsubsection{Rodent}

To establish the impact of translocation to the PAR on the \textit{Fxy} gene in \textit{M. musculus}, I sampled one-to-one orthologs of \textit{Fxy} from \textit{M. spretus} and \textit{Rattus norvegicus}, in which \textit{Fxy} is X-linked. Intronic \textit{Fxy} sequence for all species was sampled using EnsemblDb3, an open-source python tool for querying Ensembl for related sequences \citep{HuttleyEnsembldb3}. The first eight introns of \textit{Fxy} were  aligned using the Cogent3 progressive nucleotide aligner with default settings \citep{Knight2007PyCogent:Sequence}. All alignments parameters were saved in a log file for which the indel length=0.1, indel rate=1e-10. 

\subsubsection{Great Apes}

Sampling CDS and introns from the same gene is an important part of experimental design. The human genome, as well as many of the great apes are extremely well curated genomic resource. Ensembl Compara contains annotated genome-wide species comparison data where annotation of orthology is quality controlled using synteny \citep{Herrero2016EnsemblResources}. Data for the Great Apes data set was sampled from Ensembl release 104 \citep{Howe2021Ensembl2021} using EnsemblDb3 \citep{HuttleyEnsembldb3} and homologsampler \citep{HuttleyHomologsampler}, open-source python tools for querying Ensembl for related sequences. I identified one-to-one orthologs of protein coding genes in chromosome 1 of \textit{Homo sapiens} (human), \textit{Pan troglodytes} (chimpanzee) and \textit{Gorilla gorilla} (gorilla) using homologsampler. From this gene list I sampled unaligned one-to-one orthologs of exons, which corresponds to the coding sequence from the canonical transcript. To sample the introns I sampled the genes with all exons masked, masked sequence is later filtered out giving an alignment of concatenated introns. The introns are aligned by Ensembl. I aligned CDS using the Cogent3 progressive codon aligner with default settings \citep{Knight2007PyCogent:Sequence}. The CDS alignment parameters were saved into a log file for which the indel length=0.1, indel rate=1e-10. 

\subsection{Data Filtering}

Substitution models are explicitly restricted to interchanges between nucleotide states, so annotated positions linked to other mutation types were removed. This includes annotated gap characters, representing an insertion-deletion event, and simple tandem repeats, likely to evolve through strand slippage \citep{Levinson1987Slipped-strandEvolution}. For all CDS, to reduce the impact of selection, I filtered to include sites corresponding to the third codon position only. Alignments that contained less than $300$ position after filtering were not included in the analyses. Summary statistics of the number and length of both the raw and filtered alignments are shown in Table \ref{tab:seq_summary}. 

\input{figures/tables/seq_summary}

I can exemplify the filtering process using dotplots. A dotplot is a way of visualising the relatedness of biological sequences \citep{Gibbs1970TheSequences}. Using a pseudo-random number generator I selected an alignment from both the CDS and Intronic  Great Ape data sets to exemplify this filtering process, shown in Figure \ref{fig:dotplots}. In this case, the X and Y axes correspond to nucleotide sequences, each point on the plot indicates where the nucleotide is identical between the two sequences. Long stretches of identity between the sequences form a diagonal. The dotplot algorithm used to create the plots had a window of 20bp and a match was considered if $\>13$bp in the window were identical. Matches of the same + stand are shown in blue, matches of the reverse complement are shown in red. Long stretches of tandem repeats show up as large blocks of colour, visible in the unfiltered intronic dotplot, Figure \ref{fig:dotplots}a. Figure \ref{fig:dotplots}b shows the same alignment, but after the filtering process, in which these repeated sequence have been removed. 

\input{figures/diagrams/primate_dotplots}

\section{Models}

Substitution models describe the evolutionary process along the edge of a tree using a Markov process. For a nucleotide process, the possible states are the finite set of nucleotide bases $X = [A, C, G, T]$ and the state frequencies at the common ancestor is denoted $\pi$. Most commonly used are continuous-time processes, defined by a rate matrix denoted $\mathrm{Q}$. The elements of $\mathrm{Q}$ represent the instantaneous rate of change from the row label state (e.g. $A$) to a column label state (e.g. $T$). A discrete-time process is defined in terms of substitution probability matrix, denoted $\mathrm{P}$, in which elements represent the probabilities of substituting the row label state with the column label state in some time (t) interval. For a continuous-time model, the two representations are related by the following, $\mathrm{P}(t) = e^{\mathrm{Q}t}$. 

In this work I will use three continuous-time models: General Nucleotide (GN),  General Stationary (GS) and General Time Reversible (GTR); and one discrete-time model, the Barry and Hartigan (BH) model. GN is a non-stationary and non-time-reversible nucleotide model for which time-homogeneity between nodes in the tree is the only required constraint \citep{Kaehler2015}. Using this model of evolution, there are 12 distinct rates taking place at any given site. GNS is general nucleotide Markov process that only differs to GN by its constraint of stationarity. That is that the base frequencies, $\pi$, do not change through time. The formulation of GNS is provided by Von Bing Yap and Gavin Huttley (Personal Communication). GTR is a general nucleotide model that is constrained to be time-reversible and thus stationary \citep{Lanave1984ARates} and is the dominant model of choice in the field. The BH model is a general discrete-time Markov process for nucleotides \citep{Barry1987StatisticalEvolution}. The assumptions for each model can be found in table \ref{model_assumptions}.

\input{figures/tables/model_assumptions}

For a continuous-time process, a genetic distance can be given in terms of the expected number of substitutions per site in a given time interval. For a non-stationary process this requires a generalised measure \citep{Kaehler2015}. When applied to data satisfying the assumption of stationarity, this measure reduces to the standard formulation of genetic distance. As per \cite{Kaehler2015}, I denote this distance the ENS distance. 

\section{Defining Novel Statistical Measures}

\subsection{Maximum Likelihood}

ML is a well established framework for statistical inference using substitution models. Under mild conditions ML is consistent and as the as amount of data (the length of the alignment) tends to infinity, the probability of obtaining the true value of the parameters tends to one \citep{Chang1996FullConsistency}. For some data, the likelihood of a hypothesis is the probability of the data given that hypothesis. For likelihoods involving substitution models, the tree topology and the values of free parameters are these hypotheses. ML methods `fit' a model by returning the model parameters that produce the highest probability of generating the observed sequence data \citep{Edwards1972Likelihood, Felsenstein1981EvolutionaryApproach}.

ML is a powerful framework for determining whether there is significant improvement between \gls{nested} models. One `null' model is considered nested in another if it can be specified simply by imposing restrictions on the parameters of the `alternate' model.  For the models of interest, GNS is nested within GN as it can be specified by imposing the constraint of stationarity. For nested models, it is guaranteed that the likelihood for the alternate will be greater or equal to that for the null. 

A likelihood ratio test (LRT) is a method of comparing nested models. The LRT process is described in Figure \ref{fig:lrt}. An LRT is a test of whether additional components unique to the alternate model cause a significant improvement in the description of the data. Practically, this involves comparing likelihoods between models. The log-likelihood ($\ln\mathcal{L}$) for each model is obtained via fitting. The likelihood ratio (LR), is the ratio of the $\ln\mathcal{L}$ of the two models:
$$\mathrm{LR} = 2( ln L_{alt} -  ln L_{null}).$$
Suppose the null model is appropriate and the length of the alignment is adequate. In that case, the LRT statistic will be $\chi^2_{df}$ distributed with degrees freedom (df) equal to the difference in the number of free parameter between the models \citep{Lindgren1993StatisticalTheory, Silvey1975StatisticalInference, Kendall1979The2}.

\input{figures/diagrams/LRT}

\subsection{Test of Existence}

To test for the existence of mutation disequilibrium on a single edge of a tree I have developed an LRT, denoted the Test of Existence (TOE). The TOE is an LRT between the following two hypotheses: \\ $\mathbf{H_0}$: the foreground evolved according to the \textbf{GNS}, the background according to BH. \\ $\mathbf{H_1}$: the foreground evolved according to the \textbf{GN}, the background according to BH.\\ 
The two models fundamental to this test are GN and GNS. Their point of difference is that the parameters of a GNS rate matrix must satisfy $\pi\mathbf{Q}=0$, i.e., the process is constrained to be stationary (and thus in equilibrium). Consider an LRT in which GNS is the null and GN is the alternate. If we reject the null, we have some evidence that the sequences are more likely to have been generated by a process which is not yet stationary, as this is the only component unique to the alternate.

An LRT as described above is too naive for my objective to test for equilibrium in a single lineage. For a GN model to be \gls{identifiable} requires an alignment of at least three sequences \citep{Chang1996FullConsistency}. Consequently, a significant result for such an LRT will not reveal which of the taxa is causing the rejection of the null. To test for disequilibrium in a single \gls{edge} requires  modelling a continuous-time process on the edge of interest (herein the foreground edge) and assuming discrete-time processes for the other edges (herein the background edges) \citep{Verbyla2013TheSubstitution}. A discrete-time process is more general (fewer assumptions) than even the GN, making it the ideal background.

Using mixed discrete- and continuous-time Markov processes, I can test for disequilibrium on a single edge. For such an LRT, the foreground edge assumes GNS for the null, and GN for the alternate. Both hypotheses assume a Barry and Hartigan (BH) model for the background edges \citep{Barry1987StatisticalEvolution}. The modelling of the foreground edge, set apart by the assumption of stationarity, is the only point of difference between hypotheses. For this LRT, a rejection of the null means that the foreground edge was described significantly better with a non-stationary process. Such a result suggests disequilibrium precisely in the foreground edge. Herein all model fits assume mixed discrete- and continuous-time Markov processes, and a BH process is always assumed for the background edges. For brevity, I will refer to a model by the process assumed on the foreground edge.

\subsection{Tests of Equivalence of Process}

To test for equivalence of the evolutionary process I have defined two LRTs. The question of equivalence applies to two comparisons, adjacent, for the comparison of adjacent genes in the same species, an example being one half of \textit{Fxy} versus the other. The other comparison is temporal, for the comparison of one-to-one orthologs, like a gene in \textit{D. melanogaster}, and its ortholog in a closely related drosophila species.

\subsubsection{Adjacent }

For a single alignment of three taxa, a split continuous- and discrete-time model can be specified by the following set of parameters: $( \bm{Q}_{fg}, \bm{P}_{bg1}, \bm{P}_{bg2}, \pi_{0}, b ) $ where $\bm{Q}_{fg}$ is a continuous-time rate matrix describing the foreground edge,  $\bm{P}_{bg1}$ and $\bm{P}_{bg2}$ are discrete-time probability substitution matrices each describing one of the background edges, $\pi_{0}$ is the motif probabilities in the most recent common ancestor, and $b$ is the branch length corresponding to the foreground edge. 

For the adjacent Equivalence of Process test (aEOP) between two alignments aln\_1 and aln\_2, the null hypothesis constrains both alignments to have the same root motif probabilities and the same rate matrix on the foreground edge, i.e.,  \\
$\mathbf{H_0}:$ $\bm{Q}_{fg}(\text{aln\_1}) = \bm{Q}_{fg}(\text{aln\_2})$,  $\pi_0(\text{aln\_1}) = \pi_0(\text{aln\_2})$. \\
$\mathbf{H_1}:$ all parameters are independent between alignments. 

\subsubsection{Temporal }

The temporal Equivalence of Process test (tEOP) is between two edges of the same aligned sequence. In this case, there are two edges of interest, so the process is modelled with all edges as continuous time process. Such a model is specified by the following set of parameters: $( \bm{Q}_{fg1}, \bm{Q}_{fg2}, \bm{Q}_{bg}, \pi_{0}, b_{fg1}, b_{fg2}, b_{bg1}) $, where $\bm{Q}_{fg1}$ and $\bm{Q}_{fg2}$ are continuous-time rate matrices each describing on of the foreground edges,  $\bm{Q}_{bg}$ is a continuous-time rate matrix describing the background edge, $\pi_{0}$ is the motif probabilities in the most recent common ancestor, and $ b_{fg1}, b_{fg2}, b_{fg1}$ is the branch length corresponding to the foreground edge. 

For the tEOP, the null hypothesis constrains both foreground edges to have the same rate matrix, i.e., \\
$\mathbf{H_0}:$ $\bm{Q}_{fg1} = \bm{Q}_{fg2}$  \\ 
$\mathbf{H_1}:$ all parameters are independent between alignments. 

\subsection{Bootstrapping Procedure}

In order to get a robust estimate of the significance of an LRT statistic, parametric bootstraps can be used to determine the null distribution. For a given alignment the bootstrap procedure is: 
(1) simulate 100 alignments of the same length as the observed alignment using the parameters of the null, 
(2) perform the same fit of the null and the alternate hypotheses to each synthetic alignment, determining the LR for each.  
This procedure provides what the distribution of LR values would look like if the null hypothesis was true. The estimated $\hat p$-value is the proportion of the null distribution that exceeds the LR of the observed alignment. 

In order to indicate the reliability of an estimate of a statistic, confidence intervals can be computed. To get a confidence interval for a statistic the procedure is similar to bootstrapping. The difference being that simulations are done under the alternate hypothesis. The 2.5th and 97.5th percentiles of the simulated distribution indicate the boundaries of where we expect the parameter estimate to fall 95\% of the time. Thus, this is the range in which we have 95\% confidence that the true parameter falls. 

\section{Experimental Design}

To expose the the behaviour of the methods under both the null and alternate hypotheses, I complement  simulation studies with empirical analyses with prior predictions. Having defined my methods, it is only with both of these components that I can have confidence in their properties. I carefully constructed stationary simulated data sets reflecting what I consider to be edge cases. This allows me to establish the consistency of the statistics with theoretical expectations. I chose two empirical applications for which I have considerable confidence the data is a natural occurrence of the alternate hypothesis. These are cases with striking prior evidence for recent perturbations affecting: an entire genome (loss of DNA methylation in \textit{D. melanogaster}); or, a small genomic segment (\textit{Fxy} in \textit{M. musculus}). Using the behaviour of the methods in the previous steps as a benchmark, I then consider the evolution of the human genome. The experimental design is illustrated in Figure \ref{experimental}. 

\include{figures/diagrams/experimental_design}

\subsection{Simulating under the null hypothesis}

I require alignments simulated in accordance with the null process, stationary but non-reversible. One way to obtain data evolving under a GNS process (the null) is to exploit the fact that even a non-stationary process will converge to its stationary nucleotide distribution (herein $\pi_{\infty}$) as time goes to infinity. Essentially, I can obtain a GN model from real data, derive its corresponding $\pi_{\infty}$ and use those two things to define a stationary process. The process can be modified to model the background edges as discrete, and I can simulate alignments according to such a specification. This is my chosen simulation method as the resulting alignments are generated from parameters of real data. 

\subsubsection{Choosing Seed Alignments}

I must consider how the properties of the alignment used for simulation may affect the application of my methods. Natural data is not a single set of condition, but a very large set of possible conditions. I can not possibly consider all in simulated data. To overcome this, I tried to identify what I consider to be the attributes of data that might be edge cases, as they may define the boundaries of what I may encounter in nature. The potential confounders that I tried to address were the compositional skew, the level of historic disequilibrium and the stability of the process. The other natural confounder is the fact that the amount of data differs between different genomes, different sequence class, etc, and the amount of data is the length of the alignment. Using carefully diversified data allows for a thorough interrogation of my methods and determines how their properties may change when applied to different types of data. I have selected metrics to choose the seed alignments by based on their ability to measure the considered confounders. 

An important measure I require is that of non-stationarity. A direct consequence of a non-stationary process is a change in base composition over time. As follows, an indication of the degree of historical non-stationarity can be obtained from the difference in compositions between sequences in the same alignment. It is worth noting that studies of compositional data often require representation using Aitchison geometry. This representation allows for a consideration of the vulnerability of a composition to the sample it comes from. However, the composition of bases can also be considered as a probability distribution (i.e., $\pi = [\pi_T,\, \pi_G, \, \pi_C, \, \pi_A]$, such that $\pi_i$ where $i= \mathrm{T}, \mathrm{G}, \mathrm{C}, \mathrm{A}$ is the probability of observing the state $i$). In the case of comparing probability distributions, several measures can be used. My selected measure is an information theoretic measure known as Jensen-Shannon Divergence (JSD). JSD measures the similarity between probability distributions. JSD was chosen as it can accommodate multiple distributions, allowing us to apply it to more than two sequences if needed. Additionally, it has an associated true metric, satisfying important mathematical properties (e.g., the triangle inequality). 

A probability distribution also has an associated information content, measured using entropy. Entropy is a fundamental quantity that indicates, in this case, the evenness of base composition. For example, $\pi =[1,\, 0,\, 0,\, 0]$  (a single nucleotide) has zero entropy whilst  $\pi =[0.25,\, 0.25,\, 0.25,\, 0.25]$ (equifrequent nucleotides) has the maximum possible entropy for 4 states. As a measure of compositional diversity, it captures an essential feature of the mode of evolution. For example, an edge that is highly \gls{strand-asymmetric} would have lower entropy than a \gls{strand-symmetric} edge. The base composition may affect the properties of my developed methods. One example being $T_{50}$ for which base composition is used in its computation. Accordingly, the impact of compositions with different entropy is an important feature to consider in the process of method development.

I will approximate the stability of the process using the condition number of the foreground rate matrix. It is important to derive data from numerically stable processes. If a process falls within the scope of being numerically unstable, (meaning computers are poorly equipped to evaluate it using standard settings), I need to be aware of this so I can select a more numerically stable method. An indication of numerical stability is the eigenvector matrix condition number. A matrix condition number is an approximation of the worst case relative change in derivations, for a relative change in the input. For a continuous-time process, the transition rate matrix $\mathrm{Q}$, specifies the instantaneous rates of exchanges between states. $\mathrm{Q}$ is foundational to a given substitution model. The eigendecomposition of $\mathrm{Q}$ is a representation of $\mathrm{Q}$ in terms of its eigenvalues and eigenvectors. Eigendecomposition is fundamental to many methods that I am using and developing (e.g., deriving $\pi_{\infty}$). As an indication of the suitability of a matrix to decomposition, I will use the eigenvector matrix condition number to approximate the numerical stability of a process.

I chose four microbial alignments to generate synthetic data sets and refer to these as seed alignments. As previously stated, I expect that JSD, entropy, and condition number may affect the numerical and or statistical properties of the developed methods. To derive data from numerically stable processes, the seed alignments are chosen from a subset of alignments where the eigenvector condition number is low ($<2$). To capture the extent of naturally occurring diversity (measured by JSD) and compositional diversity (measured by entropy), the seed alignments chosen represent the permutations of those extremes. These are: high JSD, low entropy; high JSD, high entropy; low JSD, low entropy; low JSD, high entropy. The foreground edge was chosen as the ingroup taxa with the highest pairwise JSD with the outgroup. The attributes of each seed alignment is included in Table \ref{seed_alns} 

\input{figures/tables/seed_alns}

\subsubsection{Generating Synthetic Alignments that are Stationary, but not Reversible}
I expect the properties of my tests to be affected by the number of substitution events that distinguish the sequences in an alignment. The mathematical proof that LRT statistics are $\chi^{2}$ distributed assumes infinite data, that is the alignment is infinitely long. Although it has been shown that a few hundred base pairs can be sufficient for the $\chi^{2}$ to be accurate, this length is ultimately dependent on the number of substitution events \citep{Ota2000AppropriateParameters}. I can increase the number of events in a simulated alignment by increasing the length of the alignment.

It is necessary to determine how my methods are affected by the length of the alignment. I have done this by simulating multiple data sets differing in alignment length for all four seeds. The lengths were chosen to be representative of alignment lengths of my biological application. The shortest sequences I will be using are protein coding genes, for which the average length (using just the 3rd codon position) is about 300bp. For each of the four seeds, I generated data sets of alignment length 300, 3,000, and 30,000.   

To establish the properties of the EOP tests requires modified simulated data sets. The null distribution of the adjacent equivalence of the process test was determined using pairs of synthetic alignments. The simulated data sets are collections of alignments that were generated from the same stationary process. By definition the generating process is equivalent, so any differences in model fits represents sampling error arising from finite data. Since the alignments were generated randomly, I constructed an artificial genome by arbitrarily ordering sequences and then performed a `a sliding window' of size two to select pairs of alignments.

The null hypothesis for the temporal equivalence of process test is that the foreground edges are generated by the same process. This the not the case for the previously mentioned simulated data. To specify the null distribution I again used the four stationary seed alignments, but defined the two foreground edges have the same generating parameters. Again, for each of the four seeds I generated data sets of alignment length 300, 3,000, and 30,000

Each simulation set consisted of 1,000 synthetic alignments. The synthetic alignments are generated from the same function, derived from the parameter estimates from one seed alignment. An overview of the simulation process is described in Figure \ref{fig:simulating_alns}.

\input{figures/diagrams/simulating_alns}

\subsubsection{Method of model fitting}

It is important to establish that the way in which the models are numerically optimised is reasonable. Additionally, the process of fitting a model to an alignment is the most time-consuming method routinely used in this project. I have sought to identify the quickest possible method that yields a sufficiently maximised likelihood. For all synthetic data sets, I tested whether initialisation improved the model fitting process. The initialised method fit models in order of increasing generality (GTR, GNS, GN). Importantly, parameter estimates for each model were obtained from the previous model. The uninitialised method fit GNS and GN separately. The maximum likelihood estimates between uninitialised and initialised fits were compared for the models (e.g., uninitialised GN vs initialised GN). The time taken for each fitting process was recorded.  

\subsection{Empirical data with prior expectation of disequilibrium}

I have chosen what I expect to be natural occurrences of the alternate hypotheses as positive empirical controls. In these cases I have an expectation of the existence of disequilibrium based of perturbations with respect to mutagenesis. From the biology I have specific hypotheses about the expected form of mutation disequilibrium. The agreement between my predictions and results is an assessment on the ability of the developed methods to efficiently detect and measure mutation disequilibrium. 

\subsubsection{The loss of DNA methylation in \textit{D. melanogaster}}

DNA methylation, a well establish mutagenic force, has been close to lost in \textit{D. melanogaster}. This radical change in the process acting upon the genome is expected to have induced a high level of mutation disequilibrium. Whether there are other causes of mutation disequilibrium in the \textit{D. melanogaster} genome is unknown. However, the level of mutation disequilibrium of its sister taxa, \textit{D. simulans}, is a useful approximation to the level of background disequilibrium. From this, I have a clear hypothesis, that the \textit{D. melanogaster} genome will exhibit higher levels of mutation disequilibrium at a greater magnitude than the \textit{D. simulans} genome. DNA methylation is a genome-wide phenomenon and therefore, I further predict that this relationship will be consistent on a genome-wide scale. 
 
\subsubsection{The translocation of \textit{Fxy} in \textit{M. musculus}}

Recombination and its association with GC-biased gene conversion has been proposed to be a major force in the formation of isochores \citep{Montoya-Burgos2003RecombinationGenomes}. The translocation of \textit{Fxy} in \textit{M. musculus} such that half of the gene now resides in the only recombining part of the X chromosome is a unique natural experiment from which that conjecture may be tested. The nature of this local rearrangement leads to specific hypotheses. I predict that the translocation in itself would lead to the existence of mutation disequilibrium in the entire gene. I further predict that there will be elevated magnitude of disequilibrium in the region that has moved into the PAR, relative to the component of the gene that remains in the uniquely X region. 

\subsection{The Great Apes}

The level of mutation disequilibrium in the Great Apes data set is unknown. It is, however, an extraordinarily well curated genomic resource for which there are intronic and exonic sequence for the same genes. The application of methods to intronic data allows for a benchmark of the behaviour of methods in overwhelmingly selectively neutral data. 

\section{Algorithmic Implementation}

All methods described in this chapter, as well as all analysis presented in the results, were written using Python version 3.8. For all manipulation and analysis of biological sequence data, I used the develop branch of Cogent3 \citep{Knight2007PyCogent:Sequence}, an open-source python library. The other major libraries used were NumPy 1.20.0 \citep{VanDerWalt2011TheComputation}, SciPy 1.6.0 \citep{SciPy2001}, Accupy 0.3.4 \citep{accupy} and Plotly 5.2.1 \citep{plotly}. All version control was managed by Git. To ensure reproducibility of the results, all variables of analyses, including all seeds used for random number generation, were logged using SciTrack 2020.6.5.

The code for all computation steps of the project are available in public GitHub repositories. The scripts used for data sampling are included in KathData (https://github.com/GavinHuttley/KathData). All developed methods and other core library code is available in KathLibrary (https://github.com/GavinHuttley/KathLibrary). Script used to run analysis of data using the library code are available at KathAnalysis (https://github.com/GavinHuttley/KathAnalysis). The framework for testing was done using through pytest. The code coverage is the degree of the source code which has been exercised by the test cases. The code coverage of the core library code was X\%. 

Analyses that required model fitting were performed on the National Computational Infrastructure's (NCI) supercomputer Gadi. Gadi uses a Linux 8 operating system and analyses were run on multiple nodes comprising of 24-core Intel Xeon Scalable `Cascade Lake’ processors. All other analyses were performed on a MacBook Pro with four Intel i7 processors on which the  operating system was MacOS version 11.4. 


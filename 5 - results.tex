\chapter{Results}

% PAST TENSE when describing work that I have done and each result section should refer to the aim that it is tackling

The aim of the methods described in this thesis is to provide tools to query the disequilibrium of sequence evolution. Described in this chapter is the characterisation of the developed methods. I present the application of the test of existence and test of consistency to simulated data. Additionally, I present two new statistics for measuring the magnitude of disequilibrium. I evaluated these statistics on simulated data. Having selected statistical measures based of validation with data consistent with the null hypothesis, I applied my methods to naturally occurring cases of the alternate hypothesis aiming to validate whether they do, in fact, reflect an elevated level of disequilibrium. I report the application of my methods to two biological cases with striking prior evidence for recent perturbations affecting: an entire genome (loss of DNA methylation in \textit{D. melanogaster}); or, a small genomic segment (\textit{Fxy} in \textit{M. musculus}). I find that the methods are consistent with prior predictions made with knowledge of mechanism alone.  

\section*{Developing and validating the methods}

A process of validation was applied to each method. A simple specification of methods is not enough, it is necessary to determine how it behaves and that it behaves in a way that matches up with the expectation when how the data is generated is known. There is an infinite number of ways in which things can be out of equilibrium, but the number of ways in which they can be in equilibrium is a lot more restrictive. Since I am interested in departures from being in equilibrium, thatâ€™s the condition set that I have imposed in developing the methods. Below I describe the intent of a particular statistic, then describe how it was developed.

\subsection*{The best method of model fitting is without initialisation}

It is important that the best possible methods of model fitting are chosen. The test of existence seeks to determine whether the process is described significantly better by a non-stationary process. Formally, the test of existence is an LRT between the following hypotheses:\\ $\mathbf{H_0}$: the foreground evolved according to the \textbf{GNS}, the background according to BH. \\ $\mathbf{H_1}$: the foreground evolved according to the \textbf{GN}, the background according to BH.\\
A necessary precursor to any application of the test is to establish maximal fits of the models. Additionally, the process of fitting a model to an alignment is the most time-consuming method routinely used in my project. I have sought to identify the quickest possible method that yields a maximised likelihood. 

For this, I conducted an initialisation experiment that makes use of a property of nested models. For nested models, it is guaranteed that the likelihood for the alternate will be greater or equal to that for the null. General Time-Reversible (GTR) is the most general time-reversible process and is required for my initialisation experiment. GTR is nested in GNS, which in turn is nested in GN. When a model is not maximally fit, the optimisation methods have failed to find the global maximum. Such is referred to as a local maximum, where they are higher points elsewhere but not nearby. In such cases, getting parameter estimates from a nested model fit to use as initial estimates may aid the optimisation in escaping a local maximum. I also wish to establish whether there is a speed gain in initialised model fits compared to uninitialised fits. Theoretically, if given starting values close to the optimal, this may reduce the time it takes to get there. 

For all synthetic data sets, I tested whether initialisation improved the model fitting process. The initialised method fit models in order of increasing generality (GTR, GNS, GN). Importantly, parameter estimates for each model were obtained from the previous model. The uninitialised method fit GNS and GN separately. The maximum likelihood estimates between uninitialised and initialised fits were compared for the models (e.g., uninitialised GN vs initialised GN). The time taken for each fitting process was recorded.  

The initialisation experiment revealed that there are no intrinsic problems with the fitting process for GN or GNS. A fit was considered non-maximal if the log-likelihood from the initialised method was higher than that of the uninitialised method. There were no occurrences of non-maximal fits for any of the synthetic alignments. Initialised fits which were faster than the corresponding uninitialised fits were rare, occurring at a rate of about ~1\%. These results suggest that the best method of fitting for both models is without initialisation.

\subsection*{The LRT for Existence with parametric bootstrapping provided a robust estimation of significance}

To determine whether a given LRT statistic is significant requires establishing the appropriate null distribution. Statistical theory states that under certain conditions, the LRT statistic will be $\chi^2_{df}$ distributed with degrees freedom (df) equal to the difference in the number of free parameters between the models. In which case, one can obtain the p-values for a given LRT statistic simply from the analytical distribution. However, the behaviour of my test with real finite data is unknown. When considering the use of mixed discrete- and continuous-time Markov process is a deviation from convention, it is especially important to establish whether the test statistic is consistent with theoretical expectations. 

The distribution is closer to theoretical expectations for longer sequences, depicted in Figure \ref{fig:synthetic/lrt/197113-long_seq}. The figure shows data generated from the same high JSD, high entropy seed process, however, the results for all seeds were very similar, see appendix Figure \ref{fig:synthetic/lrt/all-seeds}. For alignments of length 300bp, shown in Figure \ref{fig:synthetic/lrt/197113-long_seq}a, the distribution of LRT statistic yields an excess of small p-values. Consequently, the data points of the Quantile-Quantile plot fall well below the diagonal line. Owing to increasing the power of the test, the distribution of p-values for longer alignments, illustrated in Figure \ref{fig:synthetic/lrt/197113-long_seq}b, is less skewed. Overall, the data points fall much closer to the diagonal line. It is worth noting that consistency with the theoretical distribution is most important for the smaller p-values. In Figure \ref{fig:synthetic/lrt/197113-long_seq}b, for quantiles corresponding to a significant test statistic (i.e. the bottom left corner where $p<0.05$), the distribution is very close to the theoretical distribution. This means that the chance of a false-positive is more or less in line with statistical expectations (5\%). Now, consider the distribution of p-values for $p>0.05$. Even though the distribution is not uniform, using the $\chi^{2}$ distribution, a non-significant result would still yield a non-significant p-value (true-negative). Thus, for some longer alignments, it may be suitable to assume the LRT statistics are $\chi^{2}$ distributed and in turn, obtain the p-value simply from analytical distribution. 

\input{figures/plots/synthetic/lrt/197113_332182_17210-long_seq}

The exploration with synthetic data clearly informs the application of my test. Firstly, the results of the initialisation experiment show there is no intrinsic fitting problems. As such, model fitting in all cases will be done without initialised parameter estimates. The results in Figure \ref{fig:synthetic/lrt/197113-long_seq} demonstrate that I cannot assume the LRT statistic to be $\chi^{2}$ distributed. As conventional asymptotic approximations to the LRT distribution are shown not to apply, significance levels will need to be assessed via a parametric bootstrap. 

\subsection*{A transformed $\nabla$ statistic exhibited robust behaviour under the null}

The statistic $\nabla$ is a measure of the speed of convergence of the actual process to equilibrium. Consider a process operating on a single edge of a phylogenetic tree for a time interval of length $t$, for which the frequencies of nucleotides at the root is $\pi_0$ and the rate matrix on the edge is $Q$. The nucleotide distribution at $t$ is $\pi(t) = \pi_{0} \cdot e^{Qt}$. For a stationary process, this simplifies to $\pi(t) = \pi_{0}$. Under weak assumptions, a non-stationary process will converge to a stationary process, for which $\pi$ remains unchanged over time. Thus, I can describe the speed of this convergence with the rate of change of $\pi(t)$. In other words, the derivative of $\pi$ with respect to $t$,
\begin{equation}
\label{eq:dpi/dt}
\frac{\partial \pi}{\partial t}(t) = \pi_{0} \cdot Q \cdot e^{Qt}.
\end{equation}

To describe the magnitude of a vector in a single value, it is natural to take its length. Accordingly, the $\nabla$ statistic is defined as follows,
\begin{equation}
\label{eq:len-dpi/dt}
\nabla = ||\frac{\partial \pi}{\partial t}(t)|| =|| \pi_{0} \cdot Q \cdot e^{Qt}||.
\end{equation}
$\nabla$ is the magnitude of the rate of change of $\pi(t)$. For a stationary process, for which by definition $\pi$ does not change over time, $\nabla = 0$. For a non-stationary process, as the process approaches equilibrium, $\nabla$ will asymptote to $0$. For a given non-stationary process that converges monotonically to equilibrium, $\nabla$ must increase the further one moves from equilibrium. The algorithm used to calculate $\nabla$ is presented in Algorithm \ref{alg:convergence}.

\input{figures/algorithms/convergence}

The $\nabla$ statistic required a transformation to address bias introduced in short sequences. Presented in Figure \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}a are the distributions of $\hat \nabla$ in simulated data sets generated by the same stationary seed, but for alignments of length 300, 3,000, and 30,000. Figure \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}a shows that not only was there more variation in $\hat \nabla$ for the shorter alignments, but the location of the mean differs between lengths. The High JSD, High Entropy seed is shown in Figure \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}a, however, this result was the same for all seeds, included in the appendix (see Figure \ref{fig:synthetic/conv/all_seeds}). Empirical applications must compare statistics between sequences of different lengths, for which the $\nabla$ statistic requires a transformation. 

\input{figures/plots/synthetic/d-conv-vs-conv/High JSD, High Entropy}

The selected transformation, denoted $\delta_\nabla$, adjusted for location only, ensuring the expected value was zero when the null was true. For a given alignment, $\hat \delta_\nabla$ is the difference between the observed $\hat \nabla$ and the mean of  $\hat \nabla$ of synthetic alignments generated under the null, put explicitly, $\delta_\nabla =  \nabla - \mu_{\nabla{null}}.$ Presented in Figure \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}b are the distributions of $\hat \delta_\nabla$ in simulated data sets again generated by the High JSD, High Entropy seed for alignments of length 300, 3,000, and 30,000. \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}b shows that the expected value of the transformed $\delta_\nabla$ statistic is close to zero when the null is true. Although there is more variation in the shorter sequences, this simple method of transformation was chosen because the statistic retains the same units as the untransformed statistic. Normality?

The $\delta_\nabla$ statistic increases with a known marker of historic disequilibrium. JSD is an information theoretic measure of the difference between probability distributions. Therefore, the JSD between two edges of a tree is an indication of the level of historic disequilibrium in one or more of the lineages. The $\delta_\nabla$ statistic has a strong positive relationship with the JSD between ingroup edges for taxa from the microbial data set, illustrated in Figure \ref{fig:microbial/d-conv/JSD}. This relationship is very encouraging as it supports that $\delta_\nabla$ is in fact measuring what it is intended to measure, mutation disequilibrium. 

\input{figures/plots/microbial/d-conv-JSD}

\subsection*{The $T_{50}$ statistic exhibited paradoxical propertied that prevented interpretation}

A seemingly natural way to describe disequilibrium in a system is by its proximity to equilibrium. As previously stated, for a process in disequilibrium, as time goes to infinity, its nucleotide distribution will converge to $\pi_\infty$, its equilibrium distribution. This can be expressed mathematically as 
$$\pi_\infty = \lim_{t \to \infty}\pi \cdot e^{\mathbf{Q}t}.$$ 
This property manifests in other situations, for example, the decay of a radioactive element. In those fields the problem of quantification is solved by expressing in terms of an arbitrarily chosen metric, half-life, the time taken until half of the element's original mass is left. $T_{50}$ is defined equivalently. ${T_{50}}$ is a measure of the distance to halfway to $\pi_\infty$, measured in terms of the expected number of substitutions. The algorithm used to calculate $T_{50}$ is presented in Algorithm \ref{alg:t50}.

\input{figures/algorithms/T50}

The algorithmic routines used in calculating the $T_{50}$ statistic were vulnerable to machine precision errors. The testing of $T_{50}$ illustrated a simple error that can arise from calculations using \gls{floating point arithmetic}. Consider the following equation, $1e16 + 1.0 - 1e16$. This should equal $1$, however, using a computer, it will evaluate to $0$. Such is a simple example of how the representation of numbers in computers can propagate errors in calculations. One test case for $T_{50}$ was, given model parameters generated by GTR, where by definition the nucleotide distribution is stationary, the returned value should be $0$. Interestingly, this test was failing. Debugging revealed that the failure was due to machine precision errors. Following this discovery, code for $T_{50}$ was changed to use accurate algorithms for floating point sums and dot products \citep{accupy}. These algorithms avoid possible imprecision by tracking multiple intermediate partial sums, a routine that does come with a modest speed loss \citep{Shewchuk1997AdaptivePredicates, Ogita2005AccurateProduct}. 

Even using the most precise algorithms, the estimation of $T_{50}$ was extremely vulnerable to sampling error. The simulated data was generated to be stationary, meaning the expectation was for $T_{50}$ to be zero, of course, this is never quite the case because of sampling error. The error was unmistakable in data generated by the High JSD, Low Entropy seed, shown in Figure \ref{fig:T50-short_long}, where the outlying values for shorter alignments completely obscure the shape of the distributions. Although most pronounced in the seed shown, higher sampling error in the shorter sequences was a feature of all seeds, shown in the appendix (see fig. (ref)). With such a pronounced error in shorter sequences, it would be difficult in empirical applications to distinguish error from signal. 

\input{figures/plots/synthetic/T50/HighJSDLowEntrpy}

$T_{50}$ estimated under the null was not robust to features of the data. In the simulated stationary data, $T_{50}$ appeared to have a bias towards being highest in the low JSD seeds. The median $\hat T_{50}$ for all simulated data sets is shown in Table \ref{t50_means}. In all but one case, when grouped by length and entropy, the median $\hat T_{50}$ for low JSD $>$ the median $\hat T_{50}$ for high JSD. Recall that JSD is used throughout as a marker of historic disequilibrium. Even though in the simulated data the foreground edge changed to be stationary, it was expected that in the high JSD data sets there would be more disequilibrium in the background edges. It is thus unexpected both that there is a pattern, and that the pattern reflects higher disequilibrium in the low JSD data. This was unexpected and conflicted with the understanding of both $T_{50}$ and JSD.  As a consequence, I did not continue to use $T_{50}$ and you will not see it in further results.

\input{figures/tables/T50_means}

\subsection*{Both tests of equivalence of process were consistent with asymptotic approximations}

An equivalence of process can be applied to two comparison, adjacent and temporal. Adjacent is the test of equivalence between neighbouring alignments of the same edge and temporal is the test between one-to-one orthologs. This is a hypothesis test for whether the process is shown to be different: analysing the concatenated alignments is the null (the process is the same); analysing them separately is the alternate (the process is different). Both tests are a comparison of likelihoods between fitting both alignments with one $\mathrm{Q}$ (null), or fitting a separate $\mathrm{Q}$ per alignment (alternate). Significance is assessed with a likelihood ratio, which again, it is necessary to determine whether asymptotic approximations apply. 

The null distribution of the adjacent equivalence of the process test was determined using pairs of synthetic alignments. The simulated data sets are collections of alignments that were generated from the same stationary process. By definition the generating process is equivalent, so any differences in model fits represents sampling error arising from finite data. Since the alignments were generated randomly, I constructed an artificial genome by arbitrarily ordering sequences and then performed a `a sliding window` of size two to select pairs of alignments.

The null hypothesis for the temporal equivalence of process test is that the foreground edges are generated by the same process. This the not the case for the previously mentioned simulated data. To specify the null distribution I again used the four stationary seed alignments, but defined the two foreground edges have the same generating parameters. I simulated data sets of corresponding length to the previously described synthetic data sets. 

Applying both tests to their respective null distributions indicated that they are both consistent with theoretical expectations, presented in Figure \ref{fig:synthetic/adj-temp_eop/HighJSDHighEntropy}. This is illustrated by Quantile-Quantile plots comparing the distribution of p-values to the uniform distribution for the temporal test (fig. \ref{fig:synthetic/adj-temp_eop/HighJSDHighEntropy}a) and the adjacent test (fig. \ref{fig:synthetic/adj-temp_eop/HighJSDHighEntropy}b). The orange line falls very close to the diagonal, demonstrating that the distribution of $\hat p-$values is almost indistinguishable from the uniform distribution.  This result identifies that in application of this test, it is suitable to assume the statistic is $\chi^2_{df}$ distributed and in turn, obtain the $p-$value from analytical distribution. Alignments of length 300 generated by the High JSD, High Entropy seed are shown, however, this result was the same for all seeds and all lengths, included in the appendix (see Fig. \ref{fig:synthetic/adj_eop/all_seeds} and \ref{fig:synthetic/temp_eop/all_seeds}).

\input{figures/plots/synthetic/adj-temp_eop/High JSD, High Entropy}

\section*{Testing the known}

Having lost major components of the DNA methylation process that remain in its sister taxa, the \textit{D. melanogaster} genome is a very useful natural experiment. DNA methylation is a well known mutagenic force. Consequently, the lack thereof in
\textit{D. melanogaster} led to the strong prediction that the \textit{D. melanogaster} genome would exhibit globally high levels of mutation disequilibrium. Because it is a sister taxa to \textit{D. melanogaster} that still methylates its DNA, \textit{D. simulans} provided a necessary point of comparison. I performed two separate analyses, each with either \textit{D. melanogaster} or \textit{D. simulans} as the foreground, and another close taxa that retains methylation, \textit{D. yakubra}, as the outgroup. The analyses were performed on the same alignments of third codon positions from orthologous protein coding genes. The aim was to determine whether the developed methods would reflect the expected form of mutation disequilibrium. The expectations were that the \textit{D. melanogaster} genome would exhibit higher levels of both existence and magnitude of mutation disequilibrium than \textit{D. simulans} and that this would be a genome wide relationship. 

The relative levels of purifying natural selection operating on the chromosomes allows for predictions of the magnitude of mutation disequilibrium in the chromosomes. For hetero-gametic sexes, you expect the chromosome that is hemizygous to be subjected to more stringent natural selection, because any recessive deleterious gene is exposed in the homozygous sex. The prediction is that with the increased magnitude of purifying selection, the rate of convergence to equilibrium should be slower. A slower rate of convergence to equilibrium leads to a higher magnitude of disequilibrium. Therefore, a further expectation was that the X chromosome would exhibit a higher magnitude of mutation disequilibrium relative to the autosomes. 

\subsubsection*{The entire \textit{D. melanogaster} genome is systematically elevated in both the existence and magnitude of mutation disequilibrium compared to its sister taxa, \textit{D. simulans}}

There is a striking elevation of the existence of mutation disequilibrium in the \textit{D. melanogaster} genome when compared to that of \textit{D. simulans}, shown in Figure \ref{fig:drosophila_lrt_qq}. Each subplot in Figure \ref{fig:drosophila_lrt_qq} shows for the observed data, as well as simulated +ve and -ve controls, the distribution of $\hat p-$ values from the test of existence compared to the uniform distribution. The controls are two synthetic cases of ground truth no disequilibrium (-ve), and ground truth there is disequilibrium (+ve). The observed data points of \textit{D. melanogaster}, shown in Figure \ref{fig:drosophila_lrt_qq}b, fall substantially further from the -ve control than that of \textit{D. simulans}, shown in \ref{fig:drosophila_lrt_qq}a. This indicates that, as specified by the test of existence, a much larger proportion of the \textit{D. melanogaster} genome is in mutation disequilibrium than \textit{D. simulans}. The specific proportion of each genome, requires correcting for multiple tests of the same hypothesis. 

\input{figures/plots/drosophila/LRT-QQ}

The resolution of the test of existence $\hat p-$ values is too low for conventional methods of multiple test corrections. The simulation results revealed that under the theoretical distribution the $\hat p-$ values are not uniformly distributed. As a result, they are unsuitable for assessing significance. A method for assessing significance, in this case, is with a parametric bootstrap. The compute time for a bootstrap with a single replicate is ${\sim} 6$ seconds. For each of the Fly data sets there are ${\sim} 4,500$ alignments. In order to satisfy the Bonferroni correction for $4,500$ alignments you would need to, with some level of precision, be able to estimate $p$ values below the altered significance threshold, $0.05/4,500 = 1.1{\times}10^{-5}$. For it to be possible to reject the null requires generating greater than $1/1.1{\times}10^{-5}$ replicates per alignment. This would take $9,009$ hours per alignment, of which there are $>9,000$. This is a prohibitive level of computation that is both impractical, and unnecessary. 

I have established an alternate strategy that takes advantage of the shape of the distribution. The challenge of correcting for multiple tests is pronounced in the space of genomics, for which \cite{Storey2003StatisticalStudies} introduced a formal procedure for estimating the false discovery rate. In their case, they produce an alternate statistic to be interpreted for an individual result, which isn't applicable here due to the low resolution of $p-$values. However, their procedure included an estimation of the fraction of an analysis which is consistent with the null hypothesis. This method takes advantage of how the $p-$values of data that is consistent with the null will be uniformly distributed, illustrated in Figure 1 of \citep{Storey2003StatisticalStudies}. Fitting a cubic spline to determine the inflection point, one can estimate the proportion of a given distribution that is uniform consistent with the null hypothesis, denoted here as $f$. Of interest to this analysis is $1 - f$, the proportion that is not consistent with the null hypothesis. The code to produce $f$ is included in the \href{https://github.com/StoreyLab/qvalue}{qvalue} R package \citep{Storey2004StrongApproach}.  

A significantly higher proportion of the \textit{D. melanogaster} genome was estimated to be in mutation disequilibrium compared to \textit{D. simulans}. The estimated proportion ($1 - f$) of the \textit{D. melanogaster} genome which is in mutation disequilibrium is 90\% compared to 50\% of the \textit{D. simulans} genome. Such a substantial difference is a compelling result. Where there is strong evidence of the recent evolution of mutation, \textit{D. melanogaster}, the vast majority of genes are in mutation disequilibrium. In a closely related species where the state of mutation has not so drastically changed, \textit{D. simulans}, only half of the genes are in mutation disequilibrium. This result supports the prediction that the existence of mutation disequilibrium would be elevated in \textit{D. melanogaster} relative to its sister taxa.

The magnitude of mutation disequilibrium, as measured by the $\delta_\nabla$  statistic, is also higher in \textit{D. melanogaster}. Figure \ref{fig:drosophila_d-conv-diff} shows a direct comparison allowed by the paired nature of the data sets. Each data point in Figure \ref{fig:drosophila_d-conv-diff} is the difference between $\hat \delta_\nabla$ for a \textit{D. melanogaster} gene and its ortholog in \textit{D. simulans}. These are genes with essentially the same function, but evolving under presumably different mutagenic environments. The distribution in Figure \ref{fig:drosophila_d-conv-diff} is right shifted, showing that most genes have a higher $\delta_\nabla$ estimate in \textit{D. melanogaster}. This result supports the prediction that the magnitude of mutation disequilibrium would be elevated in \textit{D. melanogaster} relative to its sister taxa. 

\input{figures/plots/drosophila/d-conv-diff}

The pattern of elevated mutation disequilibrium is systematic across the \textit{D. melanogaster} genome. The $\hat \delta_\nabla$ for a chromosome is shown with a horizontal line in Figure \ref{fig:drosophila_d-conv_manhattan}, where for all chromosomes analysed, \textit{D. melanogaster} clearly exceeds \textit{D. simulans}. In fact, comparatively, the \textit{D. simulans} genome appears to be centred very close to zero. The consistency of elevation throughout the genome is in accordance with prior predictions.

Strong purifying selection impedes the rate of convergence of the X chromosome. Considering that the X chromosome has fewer data points, it is evident in Figure \ref{fig:drosophila_d-conv-diff} that the \textit{D. melanogaster} X has an larger proportion of high $\hat \delta_\nabla$ than all other chromosomes shown. The median $\hat \delta_\nabla$ for chromosomes 2, 3 and X in \textit{D. melanogaster} is $0.0132$, $0.0131$ and $0.0177$ respectively. The median $\hat \delta_\nabla$ for chromosomes 2, 3 and X in \textit{D. simulans} is $0.0031$, $0.0031$ and $0.0040$ respectively. For both species the X chromosome clearly has a higher median $\hat \delta_\nabla$ than the autosomes, consistent with a slower rate of convergence and thus a higher magnitude of disequilibrium for sequence acted upon by purifying selection. 

\input{figures/plots/drosophila/d-conv_manhatten}

Link back to the aim and whether the result show what I aimed to show. 


\section*{Testing the conjectured}

Recombination and its association with GC-biased gene conversion has been proposed to be a major force in the formation of isochores \citep{Montoya-Burgos2003RecombinationGenomes}. The translocation of \textit{Fxy} in \textit{M. musculus} such that half of the gene now resides in the only recombining part of the X chromosome is a unique natural experiment from which that conjecture may be corroborated. This perturbation is predicted to have led to elevated disequilibrium in the region that has moved into the PAR, relative to the component of the gene that remains in the uniquely X region. To interrogate this prediction I used alignments of \textit{Fxy} in \textit{M. musculus} with orthologs from \textit{M. spretus} and \textit{R. norvegicus}. For all model fitting, \textit{M. musculus} was the foreground edge. I analysed the first six introns of Fxy, of which in \textit{M. musculus} the $5'$ end (introns 1-2) is X-specific, and the $3'$ end (introns 4-6) is located in the PAR. The boundary of the PAR is found in the third intron \citep{Palmer1997AMice}. 

I aimed to determine whether the current understanding of how this translocation may affect the divergence process would be  reflected by the methods. Because the entire gene had been translocated, I expected the whole gene to be in mutation disequilibrium. Considering the distinct mutagenic properties of the PAR, I aimed to see whether by using the statistic of magnitude, $\delta_\nabla$, I could detect elevated levels of disequilibrium in the PAR-located half. A specific location of the boundary is not provided, therefore, I aimed to see whether the magnitude of mutation disequilibrium locally within intron 3 would illustrate where the boundary is located. 

\subsubsection{The }





\section*{Testing the unknown}
\chapter{Results}

The aim of the methods described in this thesis are to provide tools to query the disequilibrium of sequence evolution. Described in this chapter are the characterisation of three novel methods to describe disequilibrium. Following this I present their application to three biological applications. 
x
\section{Quantifying Disequilibrium}
Three different measures of disequilibrium are described below. 

\subsection{$T_{50}$}
A natural way to describe disequilibrium in a system is by its proximity to equilibrium. For a process in disequilibrium, as time goes to infinity, it will converge to $\pi_\infty$, its equilibrium distribution. This can be expressed mathematically as $\pi_\infty = \lim_{t \to \infty}\pi \cdot e^{\mathbf{Q}t}.$ ${T_{50}}$ is a measure of the distance to halfway to $\pi_\infty$, measured in terms of the expected number of substitutions. The code to perform $T_{50}$ was written using Python.


\subsection{Convergence}

$$\nabla =x \frac{\partial \pi}{\partial t}$$

\subsection{Equivalence of Process}

\section{Simulation Study}

\subsection{Test for Disequilibrium}

 

To determine whether a given LRT statistic is significant requires establishing the appropriate null distribution. Statistical theory states that under certain conditions, the LRT statistic will be $\chi^2_{df}$ distributed with degrees freedom (df) equal to the difference in the number of free parameter between the models. In which case, one can obtain the p-values for a given LRT statistic simply from the analytical distribution. However, the behaviour of my test with real finite data is unknown. When considering the use of mixed discrete- and continuous-time Markov process is a deviation from convention, it is especially important to establish whether the test statistic is consistent with theoretical expectations. 

The distribution is closer to theoretical expectations for longer sequences, depicted in Figure \ref{fig:synthetic/lrt/197113-long_seq}. For alignments of length 300bp, shown in Figure \ref{fig:synthetic/lrt/197113-long_seq}a, the distribution of LRT statistic yields an excess of small p-values. Consequently, the data points of the Quantile-Quantile plot fall well below the diagonal line. Owing to increasing the power of the test, the distribution of p-values for longer alignments, illustrated in Figure \ref{fig:synthetic/lrt/197113-long_seq}b, is less skewed. Overall, the data points fall much closer to the diagonal line. It is worth noting that consistency with the theoretical distribution is most important for the smaller p-values. In Figure \ref{fig:synthetic/lrt/197113-long_seq}b, for quantiles corresponding to a significant test statistic (i.e. the bottom left corner where $p<0.05$), the distribution is very close to the theoretical distribution. This means that the chance of a false-positive is more or less in line with statistical expectations (5\%). Now, consider the distribution of p-values for $p>0.05$. Even though the distribution is not uniform, using the $\chi^{2}$ distribution, a non-significant result would still yield a non-significant p-value (true-negative). Thus, for some longer alignments, it may be suitable to assume the LRT statistics are $\chi^{2}$ distributed and in turn, obtain the p-value simply from analytical distribution. 



\input{figures/plots/synthetic/lrt/197113_332182_17210-long_seq}

Figure \ref{fig:synthetic/lrt/197113-long_branch} shows that increasing the branch length of sequences did not alter the distribution of LRT statistic. The distribution of LRT with and without branch scaling is depicted in Figures \ref{fig:synthetic/lrt/197113-long_branch}a and \ref{fig:synthetic/lrt/197113-long_branch}b respectively. The distributions are almost identical, which is further illustrated with a Quantile-Quantile plot comparing the distributions directly to each other, included in the \hyperref[Appendices]{Appendices} (see fig. ). It is important to note that this result is conditioned on constraints imposed on how much branch lengths could be changed. It is possible that allowing branch lengths to exceed $d=0.6$ and in turn, further increasing the number of events, would alter the distribution of the statistic.


\input{figures/plots/synthetic/lrt/197113_332182_17210-long_branch}

\subsection{Published Tests}
Squartini and Arndt 


% \input{figures/plots/synthetic/chi2/197113_332182_17210}

\subsection{$T_{50}$}

% \input{figures/plots/synthetic/T50/300bp}

% \input{figures/plots/synthetic/T50/197113_332182_17210-seq_len}

\subsection{Convergence}

% \input{figures/plots/synthetic/convergence/300bp}

\subsection{Equivalence of Process}

\section{Is the Human Genome at Equilibrium?}

The resolution of the LRT $p$ values is too low for conventional methods of multiple test corrections. The simulation results revealed that under the theoretical distribution the $p$ values are not uniformly distributed. As a result, they are unsuitable for assessing significance. A method for assessing significance in this case is with a parametric bootstrap. The compute time for a bootstrap with a single replicate is ${\sim} 6$ seconds. For each of the Primate data sets there are ${\sim} 1,300$ alignments. In order to satisfy the Bonferroni correction for $1,300$ alignments you would need to, with some level of precision, be able to estimate $p$ values below the altered significance threshold, $0.05/1,300 = 3.85{\times}10^{-5}$. For it to be possible to reject the null requires generating greater than $3.85{\times}10^{5}$ replicates per alignment. This would take $642$ hours per alignment, of which there are $2,600$. This is a prohibitive level of computation that is both impractical, and unnecessary. 

I have established an alternate strategy which takes advantage of the shape of the distribution. The challenge of correcting for multiple tests is pronounced in the space of genomics, for which \cite{Storey2003StatisticalStudies} introduced a formal procedure for estimating the false discovery rate. In their case, they produce an alternate statistic to be interpreted for an individual result, which isn't applicable here due to the low resolution of $p$ values. However, their procedure included an estimation of the fraction of an analysis which is consistent with the null hypothesis. This method takes advantage of how the $p$ values of data that is consistent with the null will be uniformly distributed, illustrated in Figure 1 of \cite{Storey2003StatisticalStudies}. Fitting a cubic spline to determine the inflection point, one can estimate the proportion of a given distribution that is uniform consistent with the null hypothesis, denoted $\hat \pi_{0}$. Of interest to this analysis is $1 -\hat \pi_{0}$, the proportion that is not consistent with the null hypothesis. The code to produce $\hat \pi_{0}$ is included in the appendices. 

\section{Is the \textit{D. melanogaster} genome further from equilibrium than \textit{D. simulans}?}
 
\section{Is the half of \textit{Fxy} located in the Psuedo-Autosomal Region (PAR) further from equilibrium than the non-PAR half in \textit{M. musculus}?}
\chapter{Results}

The aim of the methods described in this thesis is to provide tools to query the disequilibrium of sequence evolution. Described in this chapter is the characterisation of the developed methods. I present the application of the test of existence and test of consistency to simulated data. Additionally, I present two new statistics for measuring the magnitude of disequilibrium. I evaluate these statistics on simulated data. I complement this with the results of the methods applied to empirical data where I expect disequilibrium to exist.

\section*{Developing and validating the methods}

In order to ... I did ...

A process of validation was applied to each method. A simple specification of methods is not enough, it is necessary to determine how it behaves and that it behaves in a way that matches up with the expectation when how the data is generated is known. There is an infinite number of ways in which things can be out of equilibrium, but the number of ways in which they can be in equilibrium is a lot more restrictive. Since I am interested in departures from being in equilibrium, thatâ€™s the condition set that I have imposed in developing the methods. Below I describe the intent of a particular statistic, then describe how it was developed.

\subsection*{The best method of model fitting is without initialisation}

It is important that the best possible methods of model fitting are chosen. The test of existence seeks to determine whether the process is described significantly better by a non-stationary process. Formally, the test of existence is an LRT between the following hypotheses:\\ $\mathbf{H_0}$: the foreground evolved according to the \textbf{GNS}, the background according to BH. \\ $\mathbf{H_1}$: the foreground evolved according to the \textbf{GN}, the background according to BH.\\
A necessary precursor to any application of the test is to establish maximal fits of the models. Additionally, the process of fitting a model to an alignment is the most time-consuming method routinely used in my project. I have sought to identify the quickest possible method that yields a maximised likelihood. 

For this, I conducted an initialisation experiment that makes use of a property of nested models. For nested models, it is guaranteed that the likelihood for the alternate will be greater or equal to that for the null. General Time-Reversible (GTR) is the most general time-reversible process and is required for my initialisation experiment. GTR is nested in GNS, which in turn is nested in GN. When a model is not maximally fit, the optimisation methods have failed to find the global maximum. Such is referred to as a local maximum, where they are higher points elsewhere but not nearby. In such cases, getting parameter estimates from a nested model fit to use as initial estimates may aid the optimisation in escaping a local maximum. I also wish to establish whether there is a speed gain in initialised model fits compared to uninitialised fits. Theoretically, if given starting values close to the optimal, this may reduce the time it takes to get there. 

For all synthetic data sets, I tested whether initialisation improved the model fitting process. The initialised method fit models in order of increasing generality (GTR, GNS, GN). Importantly, parameter estimates for each model were obtained from the previous model. The uninitialised method fit GNS and GN separately. The maximum likelihood estimates between uninitialised and initialised fits were compared for the models (e.g., uninitialised GN vs initialised GN). The time taken for each fitting process was recorded.  

The initialisation experiment revealed that there are no intrinsic problems with the fitting process for GN or GNS. A fit was considered non-maximal if the log-likelihood from the initialised method was higher than that of the uninitialised method. There were no occurrences of non-maximal fits for any of the synthetic alignments. Initialised fits which were faster than the corresponding uninitialised fits were rare, occurring at a rate of about ~1\%. These results suggest that the best method of fitting for both models is without initialisation.

\subsection*{The LRT for Existence with parametric bootstrapping provided a robust estimation of significance}

To determine whether a given LRT statistic is significant requires establishing the appropriate null distribution. Statistical theory states that under certain conditions, the LRT statistic will be $\chi^2_{df}$ distributed with degrees freedom (df) equal to the difference in the number of free parameters between the models. In which case, one can obtain the p-values for a given LRT statistic simply from the analytical distribution. However, the behaviour of my test with real finite data is unknown. When considering the use of mixed discrete- and continuous-time Markov process is a deviation from convention, it is especially important to establish whether the test statistic is consistent with theoretical expectations. 

The distribution is closer to theoretical expectations for longer sequences, depicted in Figure \ref{fig:synthetic/lrt/197113-long_seq}. The figure shows data generated from the same high JSD, high entropy seed process, however, the results for all seeds were very similar, see appendix Figure \ref{fig:synthetic/lrt/all-seeds}. For alignments of length 300bp, shown in Figure \ref{fig:synthetic/lrt/197113-long_seq}a, the distribution of LRT statistic yields an excess of small p-values. Consequently, the data points of the Quantile-Quantile plot fall well below the diagonal line. Owing to increasing the power of the test, the distribution of p-values for longer alignments, illustrated in Figure \ref{fig:synthetic/lrt/197113-long_seq}b, is less skewed. Overall, the data points fall much closer to the diagonal line. It is worth noting that consistency with the theoretical distribution is most important for the smaller p-values. In Figure \ref{fig:synthetic/lrt/197113-long_seq}b, for quantiles corresponding to a significant test statistic (i.e. the bottom left corner where $p<0.05$), the distribution is very close to the theoretical distribution. This means that the chance of a false-positive is more or less in line with statistical expectations (5\%). Now, consider the distribution of p-values for $p>0.05$. Even though the distribution is not uniform, using the $\chi^{2}$ distribution, a non-significant result would still yield a non-significant p-value (true-negative). Thus, for some longer alignments, it may be suitable to assume the LRT statistics are $\chi^{2}$ distributed and in turn, obtain the p-value simply from analytical distribution. 

\input{figures/plots/synthetic/lrt/197113_332182_17210-long_seq}

The exploration with synthetic data clearly informs the application of my test. Firstly, the results of the initialisation experiment show there is no intrinsic fitting problems. As such, model fitting in all cases will be done without initialised parameter estimates. The results in Figure \ref{fig:synthetic/lrt/197113-long_seq} demonstrate that I cannot assume the LRT statistic to be $\chi^{2}$ distributed. As conventional asymptotic approximations to the LRT distribution are shown not to apply, significance levels will need to be assessed via a parametric bootstrap. 

\subsection*{A transformed $\nabla$ statistic exhibited robust behaviour under the null}

The statistic $\nabla$ is a measure of the speed of convergence of the actual process to equilibrium. Consider a process operating on a single edge of a phylogenetic tree for a time interval of length $t$, for which the frequencies of nucleotides at the root is $\pi_0$ and the rate matrix on the edge is $Q$. The nucleotide distribution at $t$ is $\pi(t) = \pi_{0} \cdot e^{Qt}$. For a stationary process, this simplifies to $\pi(t) = \pi_{0}$. Under weak assumptions, a non-stationary process will converge to a stationary process, for which $\pi$ remains unchanged over time. Thus, I can describe the speed of this convergence with the rate of change of $\pi(t)$. In other words, the derivative of $\pi$ with respect to $t$,
\begin{equation}
\label{eq:dpi/dt}
\frac{\partial \pi}{\partial t}(t) = \pi_{0} \cdot Q \cdot e^{Qt}.
\end{equation}

To describe the magnitude of a vector in a single value, it is natural to take its length. Accordingly, the $\nabla$ statistic is defined as follows,
\begin{equation}
\label{eq:len-dpi/dt}
\nabla = ||\frac{\partial \pi}{\partial t}(t)|| =|| \pi_{0} \cdot Q \cdot e^{Qt}||.
\end{equation}
$\nabla$ is the magnitude of the rate of change of $\pi(t)$. For a stationary process, for which by definition $\pi$ does not change over time, $\nabla = 0$. For a non-stationary process, as the process approaches equilibrium, $\nabla$ will asymptote to $0$. For a given non-stationary process that converges monotonically to equilibrium, $\nabla$ must increase the further one moves from equilibrium. The algorithm used to calculate $\nabla$ is presented in Algorithm \ref{alg:convergence}.

\input{figures/algorithms/convergence}

The $\nabla$ statistic required a transformation to address bias introduced in short sequences. Presented in Figure \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}a are the distributions of $\hat \nabla$ in simulated data sets generated by the same stationary seed, but for alignments of length 300, 3,000, and 30,000. Figure \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}a shows that not only was there more variation in $\hat \nabla$ for the shorter alignments, but the location of the mean differs between lengths. The High JSD, High Entropy seed is shown in Figure \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}a, however, this result was the same for all seeds, included in the appendix (see Figure \ref{fig:synthetic/conv/all_seeds}). Empirical applications must compare statistics between sequences of different lengths, for which the $\nabla$ statistic requires a transformation. 

The selected transformation, denoted $\delta_\nabla$, adjusted for location only, ensuring the expected value was zero when the null was true. For a given alignment, $\hat \delta_\nabla$ is the difference between the observed $\hat \nabla$ and the mean of  $\hat \nabla$ of synthetic alignments generated under the null, put explicitly, $\delta_\nabla =  \nabla - \mu_{\nabla{null}}.$ Presented in Figure \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}b are the distributions of $\hat \delta_\nabla$ in simulated data sets again generated by the High JSD, High Entropy seed for alignments of length 300, 3,000, and 30,000. \ref{fig:synthetic/d-conv-vs-conv/HighJSDHighEntropy}b shows that the expected value of the transformed $\delta_\nabla$ statistic is close to zero when the null is true. Although there is more variation in the shorter sequences, this simple method of transformation was chosen because the statistic retains the same units as the untransformed statistic. Normality?

\input{figures/plots/synthetic/d-conv-vs-conv/High JSD, High Entropy}

The $\delta_\nabla$ statistic increases with a known marker of historic disequilibrium. JSD is an information theoretic measure of the difference between probability distributions. Therefore, the JSD between two edges of a tree is an indication of the level of historic disequilibrium in one or more of the lineages. The $\delta_\nabla$ statistic has a strong positive relationship with the JSD between ingroup edges for taxa from the microbial data set, illustrated in Figure \ref{fig:microbial/d-conv/JSD}. This relationship is very encouraging as it supports that $\delta_\nabla$ is in fact measuring what it is intended to measure, mutation disequilibrium. 

\input{figures/plots/microbial/d-conv-JSD}

\subsection*{The $T_{50}$ statistic exhibited paradoxical properties that prevent interpretation}

A natural way to describe disequilibrium in a system is by its proximity to equilibrium. As previously stated, for a process in disequilibrium, as time goes to infinity, its nucleotide distribution will converge to $\pi_\infty$, its equilibrium distribution. This can be expressed mathematically as 
$$\pi_\infty = \lim_{t \to \infty}\pi \cdot e^{\mathbf{Q}t}.$$ 
This property manifests in other situations, for example, the decay of a radioactive element. In those fields the problem of quantification is solved by expressing in terms of an arbitrarily chosen metric, half-life, the time taken until half of the element's original mass is left. ${T_{50}}$ is defined equivalently.

${T_{50}}$ is a measure of the distance to halfway to $\pi_\infty$, measured in terms of the expected number of substitutions. 

Put in that for a process in equilibrium it literally is zero because it is a good point.

\input{figures/algorithms/T50}

\subsection*{Both tests of equivalence of process were consistent with asymptotic approximations}

An equivalence of process can be applied to two comparison, adjacent and temporal. Adjacent is the test of equivalence between neighbouring alignments of the same edge and temporal is the test between one-to-one orthologs. This is a hypothesis test for whether the process is shown to be different: analysing the concatenated alignments is the null (the process is the same); analysing them separately is the alternate (the process is different). Both tests are a comparison of likelihoods between fitting both alignments with one $\mathrm{Q}$ (null), or fitting a separate $\mathrm{Q}$ per alignment (alternate). Significance is assessed with a likelihood ratio, which again, it is necessary to determine whether asymptotic approximations apply. 

The null distribution of the adjacent equivalence of process test was determined using pairs of synthetic alignments. The simulated data sets are collections of alignments that were generated from the same stationary process. By definition the generating process is equivalent, so any differences in model fits represents sampling error arising from finite data. Since the alignments were generated randomly, I constructed an artificial genome by arbitrarily ordering sequences and then performed a `a sliding window` of size two to select pairs of alignments.

The null hypothesis for the temporal equivalence of process test is that the foreground edges are generated by the same process. This the not the case for the previously mentioned simulated data. To specify the null distribution I again used the four stationary seed alignments, but defined the two foreground edges have the same generating parameters. I simulated data sets of corresponding length to the previously described synthetic data sets. 

Applying both tests to their respective null distributions indicated that they are both consistent with theoretical expectations, presented in Figure \ref{fig:synthetic/adj-temp_eop/HighJSDHighEntropy}. This is illustrated by Quantile-Quantile plots comparing the distribution of p-values to the uniform distribution for the temporal test (fig. \ref{fig:synthetic/adj-temp_eop/HighJSDHighEntropy}a) and the adjacent test (fig. \ref{fig:synthetic/adj-temp_eop/HighJSDHighEntropy}b). The orange line falls very close to the diagonal, demonstrating that the distribution of $\hat p-$values is almost indistinguishable from the uniform distribution.  This result identifies that in application of this test, it is suitable to assume the statistic is $\chi^2_{df}$ distributed and in turn, obtain the $p-$value from analytical distribution. Alignments of length 300 generated by the High JSD, High Entropy seed are shown, however, this result was the same for all seeds and all lengths, included in the appendix (see Fig. \ref{fig:synthetic/temp_eop/all_seeds} and \ref{fig:synthetic/temp_eop/all_seeds}).

\input{figures/plots/synthetic/adj-temp_eop/High JSD, High Entropy}


\section*{Benchmarking methods against empirical application with strong prior evidence of mutation disequilibrium}

 In order to have confidence that my methods are robustly measuring mutation disequilibrium, I applied them to cases where empirical evidence clearly indicates the presence of a mutator. Simulations are important, but are ultimately limited by the fact that data is generated by a model and not by nature. They can indicate that the methods do not detect mutation disequilibrium that isn't there, but prove nothing about whether the methods can detect mutation disequilibrium that is there. In this section I apply my methods to two positive empirical controls with striking prior evidence for recent perturbations affecting: an entire genome (loss of DNA methylation in \textit{D. melanogaster}); or, a small genomic segment (\textit{Fxy} in \textit{M. musculus}). I find that the methods are consistent with prior predictions made with knowledge of mechanism alone. 


\subsection*{The \textit{ D. melanogaster} genome has a global elevation in mutation disequilibrium in a comparison to  \textit{D. simulans}}

\input{figures/plots/drosophila/LRT-QQ}

\input{figures/plots/drosophila/d-conv_manhatten}



\section*{Is the human genome at equilibrium?}